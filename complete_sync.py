#!/usr/bin/env python3
"""
å®Œæ•´çš„è‚¡ç¥¨æ•°æ®åŒæ­¥è„šæœ¬ - å•æ–‡ä»¶ç‰ˆæœ¬
åŒ…å«ï¼šæ•°æ®è·å– -> æ•°æ®å¤„ç† -> é£ä¹¦åŒæ­¥
ç›´æ¥è°ƒç”¨å‡½æ•°ï¼Œé¿å…subprocessçš„ç½‘ç»œé—®é¢˜
"""

import akshare as ak
import pandas as pd
import numpy as np
import requests
import json
import time
import os
from datetime import datetime
from typing import Optional, Dict, Any
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('complete_sync.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# é£ä¹¦åº”ç”¨é…ç½®
APP_ID = "cli_a8351c888cac9013"
APP_SECRET = "gIcB8ViEFF478MsW0cJQ2efsLUtNrzZa"
BASE_URL = "https://ydetzdeqyc.feishu.cn/base/U3iYbe8cGaBrLEso6jMctMVgnVb"
TABLE_ID = "tbl29O0osz3dn74L"

# æ–‡ä»¶è·¯å¾„é…ç½®
EXCEL_FILE = 'fromyouwei.xlsx'
UPDATED_EXCEL_FILE = 'fromyouwei_updated.xlsx'
PRICE_DATA_FILE = 'data/all_stock_price.csv'
EPS_DATA_FILE = 'data/è‚¡ç¥¨ä»£ç _EPS.csv'


class FeishuAuth:
    """é£ä¹¦APIè®¤è¯ç±»ï¼Œè´Ÿè´£tokenè·å–å’Œç®¡ç†"""
    
    def __init__(self, app_id: str = None, app_secret: str = None):
        self.app_id = app_id or APP_ID
        self.app_secret = app_secret or APP_SECRET
        self.tenant_access_token: Optional[str] = None
        self.token_expires_at: int = 0
        self.base_url = "https://open.feishu.cn/open-apis"
        
    def get_tenant_access_token(self, force_refresh: bool = False) -> str:
        """è·å–tenant_access_token"""
        current_time = int(time.time())
        if (not force_refresh and 
            self.tenant_access_token and 
            current_time < self.token_expires_at - 300):
            return self.tenant_access_token
            
        url = f"{self.base_url}/auth/v3/tenant_access_token/internal"
        payload = {
            "app_id": self.app_id,
            "app_secret": self.app_secret
        }
        headers = {'Content-Type': 'application/json'}
        
        try:
            response = requests.post(url, json=payload, headers=headers)
            response.raise_for_status()
            
            result = response.json()
            if result.get("code") != 0:
                raise Exception(f"è·å–tokenå¤±è´¥: {result.get('msg', 'Unknown error')}")
            
            self.tenant_access_token = result["tenant_access_token"]
            expires_in = result.get("expire", 7200)
            self.token_expires_at = current_time + expires_in
            
            logger.info(f"âœ… æˆåŠŸè·å–tokenï¼Œæœ‰æ•ˆæœŸ: {expires_in}ç§’")
            return self.tenant_access_token
            
        except Exception as e:
            logger.error(f"âŒ è·å–tokenå¤±è´¥: {str(e)}")
            raise


def get_stock_price_data_with_retry(max_retries=3, delay=2):
    """å¸¦é‡è¯•æœºåˆ¶çš„è‚¡ç¥¨ä»·æ ¼æ•°æ®è·å–"""
    for attempt in range(max_retries):
        try:
            logger.info(f"å°è¯•è·å–è‚¡ç¥¨æ•°æ® (ç¬¬{attempt + 1}/{max_retries}æ¬¡)...")
            stock_df = ak.stock_zh_a_spot_em()
            logger.info(f"æˆåŠŸè·å–{len(stock_df)}æ¡è‚¡ç¥¨æ•°æ®")
            return stock_df
        
        except Exception as e:
            logger.error(f"ç¬¬{attempt + 1}æ¬¡è·å–å¤±è´¥: {str(e)}")
            if attempt < max_retries - 1:
                logger.info(f"ç­‰å¾…{delay}ç§’åé‡è¯•...")
                time.sleep(delay)
                delay *= 2
            else:
                logger.error("æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
                raise e


def get_eps_data_with_retry(stock_code, max_retries=3, delay=1):
    """å¸¦é‡è¯•æœºåˆ¶çš„EPSæ•°æ®è·å–"""
    for attempt in range(max_retries):
        try:
            stock_df = ak.stock_profit_forecast_ths(symbol=stock_code, indicator="é¢„æµ‹å¹´æŠ¥æ¯è‚¡æ”¶ç›Š")
            stock_df['è‚¡ç¥¨ä»£ç '] = stock_code
            return stock_df
        
        except Exception as e:
            logger.warning(f"è‚¡ç¥¨ {stock_code} ç¬¬{attempt + 1}æ¬¡è·å–å¤±è´¥: {str(e)}")
            if attempt < max_retries - 1:
                time.sleep(delay)
                delay *= 1.5
            else:
                logger.warning(f"è‚¡ç¥¨ {stock_code} æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")
                return pd.DataFrame()


def step1_get_stock_price():
    """æ­¥éª¤1: è·å–è‚¡ç¥¨ä»·æ ¼æ•°æ®"""
    logger.info("=" * 50)
    logger.info("æ­¥éª¤1: è·å–è‚¡ç¥¨ä»·æ ¼æ•°æ®")
    logger.info("=" * 50)
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ•°æ®æ–‡ä»¶
    if os.path.exists(PRICE_DATA_FILE):
        logger.info(f"âœ… å‘ç°å·²å­˜åœ¨çš„ä»·æ ¼æ•°æ®æ–‡ä»¶: {PRICE_DATA_FILE}")
        try:
            df_check = pd.read_csv(PRICE_DATA_FILE)
            logger.info(f"âœ… ä½¿ç”¨ç°æœ‰ä»·æ ¼æ•°æ® ({len(df_check)} æ¡è®°å½•)")
            return True
        except Exception as e:
            logger.warning(f"âš ï¸ ç°æœ‰æ•°æ®æ–‡ä»¶æŸåï¼Œé‡æ–°è·å–: {e}")
    
    try:
        os.makedirs('data', exist_ok=True)
        stock_zh_a_spot_em_df = get_stock_price_data_with_retry()
        stock_zh_a_spot_em_df.to_csv(PRICE_DATA_FILE, index=False)
        logger.info(f"âœ… è‚¡ç¥¨ä»·æ ¼æ•°æ®å·²ä¿å­˜åˆ°: {PRICE_DATA_FILE}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ è·å–è‚¡ç¥¨ä»·æ ¼æ•°æ®å¤±è´¥: {str(e)}")
        return False


def step2_get_eps_data():
    """æ­¥éª¤2: è·å–EPSæ•°æ®"""
    logger.info("=" * 50)
    logger.info("æ­¥éª¤2: è·å–EPSé¢„æµ‹æ•°æ®")
    logger.info("=" * 50)
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰EPSæ•°æ®æ–‡ä»¶
    if os.path.exists(EPS_DATA_FILE):
        logger.info(f"âœ… å‘ç°å·²å­˜åœ¨çš„EPSæ•°æ®æ–‡ä»¶: {EPS_DATA_FILE}")
        try:
            df_check = pd.read_csv(EPS_DATA_FILE)
            logger.info(f"âœ… ä½¿ç”¨ç°æœ‰EPSæ•°æ® ({len(df_check)} æ¡è®°å½•)")
            return True
        except Exception as e:
            logger.warning(f"âš ï¸ ç°æœ‰EPSæ•°æ®æ–‡ä»¶æŸåï¼Œé‡æ–°è·å–: {e}")
    
    stock_list = ['002156', '002837', '300229', '600249', '002230', '688111', '603019', '300496', '600519']
    
    all_stocks_esp_df = pd.DataFrame()
    successful_count = 0
    
    for i, stock_code in enumerate(stock_list):
        logger.info(f"æ­£åœ¨å¤„ç†è‚¡ç¥¨ {stock_code} ({i+1}/{len(stock_list)})...")
        
        try:
            stock_data = get_eps_data_with_retry(stock_code)
            
            if not stock_data.empty:
                all_stocks_esp_df = pd.concat([all_stocks_esp_df, stock_data], ignore_index=True)
                successful_count += 1
                logger.info(f"âœ… æˆåŠŸè·å–è‚¡ç¥¨ {stock_code} çš„é¢„æµ‹æ¯è‚¡æ”¶ç›Šæ•°æ®")
            else:
                logger.warning(f"âš ï¸ è‚¡ç¥¨ {stock_code} æ•°æ®è·å–å¤±è´¥ï¼Œè·³è¿‡")
            
            time.sleep(0.5)
            
        except Exception as e:
            logger.error(f"âŒ è·å–è‚¡ç¥¨ {stock_code} æ•°æ®æ—¶å‡ºç°å¼‚å¸¸: {e}")
    
    if not all_stocks_esp_df.empty:
        all_stocks_esp_df.to_csv(EPS_DATA_FILE, index=False, encoding='utf-8-sig')
        logger.info(f"âœ… EPSæ•°æ®å·²ä¿å­˜åˆ°: {EPS_DATA_FILE}")
        logger.info(f"ğŸ“Š å…±å¤„ç†äº† {len(stock_list)} åªè‚¡ç¥¨ï¼ŒæˆåŠŸè·å– {successful_count} åªè‚¡ç¥¨çš„æ•°æ®ï¼Œæ€»è®¡ {len(all_stocks_esp_df)} æ¡è®°å½•")
        return True
    else:
        logger.error("âŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•EPSæ•°æ®")
        return False


def step3_process_excel_data():
    """æ­¥éª¤3: å¤„ç†Excelæ•°æ®"""
    logger.info("=" * 50)
    logger.info("æ­¥éª¤3: å¤„ç†Excelæ•°æ®ï¼Œè®¡ç®—è´¢åŠ¡æŒ‡æ ‡")
    logger.info("=" * 50)
    
    try:
        # è¯»å–Excelæ–‡ä»¶
        df_excel = pd.read_excel(EXCEL_FILE)
        logger.info(f"è¯»å–Excelæ–‡ä»¶: {len(df_excel)} è¡Œæ•°æ®")
        
        # è¯»å–è‚¡ç¥¨ä»·æ ¼æ•°æ®
        df_price = pd.read_csv(PRICE_DATA_FILE)
        df_price['ä»£ç '] = df_price['ä»£ç '].astype(str).str.zfill(6)
        
        # è¯»å–EPSæ•°æ®
        df_eps = pd.read_csv(EPS_DATA_FILE)
        df_eps_2025 = df_eps[df_eps['å¹´åº¦'] == 2025].copy()
        df_eps_2025['è‚¡ç¥¨ä»£ç '] = df_eps_2025['è‚¡ç¥¨ä»£ç '].astype(str).str.zfill(6)
        
        # åˆ›å»ºæŸ¥æ‰¾å­—å…¸
        price_dict = dict(zip(df_price['ä»£ç '], df_price['æœ€æ–°ä»·']))
        market_cap_dict = dict(zip(df_price['ä»£ç '], df_price['æ€»å¸‚å€¼']))
        eps_dict = dict(zip(df_eps_2025['è‚¡ç¥¨ä»£ç '], df_eps_2025['å‡å€¼']))
        
        # å¤„ç†æ¯ä¸€è¡Œ
        def process_row(row):
            ticker = str(row['Ticker'])[:6]
            
            # è·å–æ•°æ®
            current_price = price_dict.get(ticker, np.nan)
            eps_mean = eps_dict.get(ticker, np.nan)
            
            # è®¡ç®—æŒ‡æ ‡
            pe_2025e = current_price / eps_mean if eps_mean and eps_mean != 0 and not np.isnan(eps_mean) else np.nan
            market_cap = market_cap_dict.get(ticker, np.nan)
            market_cap_bn = market_cap / 100000000 if not np.isnan(market_cap) else np.nan
            
            target_low = row['Target Low']
            target_high = row['Target High']
            mid_target = (target_low + target_high) / 2 if not np.isnan(target_low) and not np.isnan(target_high) else np.nan
            potential_upside = round((mid_target / current_price - 1) * 100, 1) if not np.isnan(current_price) and not np.isnan(mid_target) and current_price != 0 else np.nan
            
            # æ›´æ–°è¡Œæ•°æ®
            row['Current Price'] = current_price
            row['EPS (2025E)'] = eps_mean
            row['PE (2025E)'] = pe_2025e
            row['Market Cap (CNY bn)'] = market_cap_bn
            row['Mid Target'] = mid_target
            row['Potential Upside %'] = potential_upside
            # æ·»åŠ Last Updatedå­—æ®µ - ä½¿ç”¨åŒ—äº¬æ—¶é—´
            from datetime import timezone, timedelta
            beijing_tz = timezone(timedelta(hours=8))
            beijing_time = datetime.now(beijing_tz)
            row['Last Updated'] = beijing_time.strftime('%Y-%m-%d %H:%M:%S CST')
            
            return row
        
        # åº”ç”¨å¤„ç†å‡½æ•°
        df_excel = df_excel.apply(process_row, axis=1)
        
        # ä¿å­˜æ›´æ–°åçš„Excelæ–‡ä»¶
        df_excel.to_excel(UPDATED_EXCEL_FILE, index=False)
        logger.info(f"âœ… Excelæ–‡ä»¶å·²æˆåŠŸæ›´æ–°å¹¶ä¿å­˜åˆ°: {UPDATED_EXCEL_FILE}")
        logger.info(f"å¤„ç†äº† {len(df_excel)} è¡Œæ•°æ®")
        return True
        
    except Exception as e:
        logger.error(f"âŒ å¤„ç†Excelæ•°æ®å¤±è´¥: {str(e)}")
        return False


def step4_sync_to_feishu():
    """æ­¥éª¤4: åŒæ­¥åˆ°é£ä¹¦"""
    logger.info("=" * 50)
    logger.info("æ­¥éª¤4: åŒæ­¥æ•°æ®åˆ°é£ä¹¦")
    logger.info("=" * 50)
    
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰user_access_tokenç¯å¢ƒå˜é‡
        user_access_token = os.getenv('FEISHU_USER_TOKEN')
        if not user_access_token:
            logger.warning("âš ï¸ æœªè®¾ç½®FEISHU_USER_TOKENç¯å¢ƒå˜é‡ï¼Œè·³è¿‡é£ä¹¦åŒæ­¥")
            logger.info("ğŸ’¡ è¦å¯ç”¨é£ä¹¦åŒæ­¥ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡: export FEISHU_USER_TOKEN=your_token")
            return True
        
        # å¯¼å…¥é£ä¹¦API
        try:
            import lark_oapi as lark
            from lark_oapi.api.bitable.v1 import (
                ListAppTableRecordRequest, BatchUpdateAppTableRecordRequest,
                BatchUpdateAppTableRecordRequestBody, AppTableRecord
            )
        except ImportError:
            logger.error("âŒ ç¼ºå°‘lark_oapiä¾èµ–ï¼Œè¯·è¿è¡Œ: pip install lark_oapi")
            return False
        
        # è¯»å–å¤„ç†åçš„Excelæ•°æ®
        df = pd.read_excel(UPDATED_EXCEL_FILE)
        logger.info(f"è¯»å–æ›´æ–°åçš„Excelæ•°æ®: {len(df)} è¡Œ")
        
        # é£ä¹¦é…ç½®
        app_token = BASE_URL.split('/')[-1]
        table_id = TABLE_ID
        
        logger.info("ğŸ”— åˆ›å»ºé£ä¹¦å®¢æˆ·ç«¯")
        client = lark.Client.builder() \
            .app_id(APP_ID) \
            .app_secret(APP_SECRET) \
            .enable_set_token(True) \
            .log_level(lark.LogLevel.INFO) \
            .build()
        
        option = lark.RequestOption.builder().user_access_token(user_access_token).build()
        
        # æ•°æ®å¤„ç†å‡½æ•°
        def fix_field_mapping(excel_fields):
            """ä¿®å¤å­—æ®µæ˜ å°„å’Œæ•°æ®ç±»å‹"""
            fixed_fields = {}
            
            field_mapping = {
                'Sector/Theme': 'Sector|Theme',
                'Source / Link': 'Source | Link',
            }
            
            number_fields = {
                'Current Price', 'Market Cap (CNY bn)', 'Safe Buy Low', 'Safe Buy High', 
                'Extreme Safe', 'Target Low', 'Target High', 'Mid Target', 
                'Potential Upside %', 'Stop Loss'
            }
            
            text_fields = {
                'Ticker', 'Name', 'Sector|Theme', 'PE (2025E)', 'EPS (2025E)', 
                'Source | Link', 'Notes', 'Last Updated'
            }
            
            for field_name, value in excel_fields.items():
                feishu_field_name = field_mapping.get(field_name, field_name)
                
                if value is None or (isinstance(value, float) and str(value).lower() == 'nan'):
                    if feishu_field_name in number_fields:
                        fixed_value = 0
                    else:
                        fixed_value = ""
                elif feishu_field_name in number_fields:
                    try:
                        if isinstance(value, str):
                            fixed_value = float(value) if '.' in value else int(value)
                        else:
                            fixed_value = float(value)
                    except (ValueError, TypeError):
                        fixed_value = 0
                elif feishu_field_name in text_fields:
                    fixed_value = str(value)
                else:
                    fixed_value = str(value)
                
                fixed_fields[feishu_field_name] = fixed_value
            
            return fixed_fields
        
        # è½¬æ¢Excelæ•°æ®ä¸ºé£ä¹¦æ ¼å¼
        excel_records = []
        for _, row in df.iterrows():
            fields = {}
            for col in df.columns:
                value = row[col]
                # å¤„ç†NaNå’ŒNoneå€¼
                if pd.isna(value) or value is None:
                    fields[col] = ""
                elif isinstance(value, float) and str(value).lower() == 'nan':
                    fields[col] = ""
                else:
                    fields[col] = value
            excel_records.append({'fields': fields})
        
        # ä¿®å¤å­—æ®µæ˜ å°„
        fixed_records = []
        for record in excel_records:
            original_fields = record.get('fields', {})
            fixed_fields = fix_field_mapping(original_fields)
            fixed_records.append({'fields': fixed_fields})
        
        logger.info(f"ğŸ“ å¤„ç†äº† {len(fixed_records)} æ¡è®°å½•")
        
        # è·å–ç°æœ‰è®°å½•
        logger.info("ğŸ“‹ è·å–ç°æœ‰é£ä¹¦è®°å½•")
        list_request = ListAppTableRecordRequest.builder() \
            .app_token(app_token) \
            .table_id(table_id) \
            .page_size(500) \
            .build()
            
        list_response = client.bitable.v1.app_table_record.list(list_request, option)
        
        if not list_response.success():
            logger.error(f"âŒ è·å–é£ä¹¦è®°å½•å¤±è´¥: {list_response.code}, {list_response.msg}")
            return False
            
        existing_records = list_response.data.items or []
        logger.info(f"ğŸ“‹ è·å–åˆ°é£ä¹¦è®°å½•: {len(existing_records)} æ¡")
        
        # åˆ›å»ºæ˜ å°„
        existing_map = {}
        for record in existing_records:
            ticker = record.fields.get('Ticker')
            if ticker:
                existing_map[str(ticker)] = record.record_id
        
        logger.info(f"ğŸ—‚ï¸ åˆ›å»ºæ˜ å°„: {len(existing_map)} æ¡")
        
        # æ›´æ–°è®°å½•
        logger.info("ğŸ”„ å¼€å§‹æ›´æ–°è®°å½•")
        success_count = 0
        fail_count = 0
        
        for i, record in enumerate(fixed_records, 1):
            fixed_fields = record['fields']
            ticker = fixed_fields.get('Ticker', '')
            name = fixed_fields.get('Name', 'æœªçŸ¥')
            
            logger.info(f"å¤„ç† {i}/{len(fixed_records)}: {ticker} ({name})")
            
            if ticker in existing_map:
                record_id = existing_map[ticker]
                
                update_request = BatchUpdateAppTableRecordRequest.builder() \
                    .app_token(app_token) \
                    .table_id(table_id) \
                    .request_body(BatchUpdateAppTableRecordRequestBody.builder()
                        .records([
                            AppTableRecord.builder()
                                .record_id(record_id)
                                .fields(fixed_fields)
                                .build()
                        ])
                        .build()) \
                    .build()
                
                update_response = client.bitable.v1.app_table_record.batch_update(update_request, option)
                
                if update_response.success():
                    success_count += 1
                    logger.info(f"  âœ… æ›´æ–°æˆåŠŸ")
                else:
                    fail_count += 1
                    logger.error(f"  âŒ æ›´æ–°å¤±è´¥: {update_response.code} - {update_response.msg}")
            else:
                logger.warning(f"  âš ï¸ è·³è¿‡: Ticker {ticker} åœ¨é£ä¹¦è¡¨æ ¼ä¸­ä¸å­˜åœ¨")
        
        # ç»“æœæŠ¥å‘Š
        logger.info("ğŸ‰ é£ä¹¦åŒæ­¥å®Œæˆ!")
        logger.info(f"âœ… æˆåŠŸ: {success_count} æ¡")
        logger.info(f"âŒ å¤±è´¥: {fail_count} æ¡")
        logger.info(f"ğŸ“Š æ€»è®¡: {len(fixed_records)} æ¡")
        
        return success_count > 0
        
    except Exception as e:
        logger.error(f"âŒ åŒæ­¥åˆ°é£ä¹¦å¤±è´¥: {str(e)}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return False


def main():
    """ä¸»æ‰§è¡Œæµç¨‹"""
    start_time = datetime.now()
    logger.info("=" * 60)
    logger.info("ğŸš€ å¼€å§‹å®Œæ•´çš„è‚¡ç¥¨æ•°æ®åŒæ­¥æµç¨‹")
    logger.info(f"å¼€å§‹æ—¶é—´: {start_time}")
    logger.info("=" * 60)
    
    success_steps = 0
    total_steps = 4
    
    try:
        # æ­¥éª¤1: è·å–è‚¡ç¥¨ä»·æ ¼æ•°æ®
        if step1_get_stock_price():
            success_steps += 1
        else:
            logger.error("è·å–ä»·æ ¼æ•°æ®å¤±è´¥ï¼Œç»ˆæ­¢æµç¨‹")
            return False
        
        # æ­¥éª¤2: è·å–EPSæ•°æ®
        if step2_get_eps_data():
            success_steps += 1
        else:
            logger.error("è·å–EPSæ•°æ®å¤±è´¥ï¼Œç»ˆæ­¢æµç¨‹")
            return False
        
        # æ­¥éª¤3: å¤„ç†Excelæ•°æ®
        if step3_process_excel_data():
            success_steps += 1
        else:
            logger.error("å¤„ç†Excelæ•°æ®å¤±è´¥ï¼Œç»ˆæ­¢æµç¨‹")
            return False
        
        # æ­¥éª¤4: åŒæ­¥åˆ°é£ä¹¦
        if step4_sync_to_feishu():
            success_steps += 1
        else:
            logger.error("åŒæ­¥åˆ°é£ä¹¦å¤±è´¥ï¼Œç»ˆæ­¢æµç¨‹")
            return False
        
        # å®Œæˆ
        end_time = datetime.now()
        duration = end_time - start_time
        
        logger.info("=" * 60)
        logger.info("ğŸ‰ å®Œæ•´çš„æ•°æ®åŒæ­¥æµç¨‹æ‰§è¡ŒæˆåŠŸï¼")
        logger.info(f"æˆåŠŸæ­¥éª¤: {success_steps}/{total_steps}")
        logger.info(f"æ€»ç”¨æ—¶: {duration.total_seconds():.2f}ç§’")
        logger.info(f"ç»“æŸæ—¶é—´: {end_time}")
        logger.info("=" * 60)
        
        return True
        
    except Exception as e:
        logger.error(f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {str(e)}")
        return False


if __name__ == "__main__":
    import sys
    success = main()
    sys.exit(0 if success else 1)